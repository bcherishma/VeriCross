import torch
import re
import spacy
from typing import List, Dict, Optional, Tuple
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
from sentence_transformers import SentenceTransformer
import bert_score
from langdetect import detect, DetectorFactory, LangDetectException

# Set seed for consistent language detection
DetectorFactory.seed = 0

class HallucinationDetector:
    def __init__(self):
        print("ðŸš€ Initializing VeriCross Enterprise Engine...")
        # 1. CROSS-LINGUAL NLI (mDeBERTa-v3) - High performance for HI/EN
        self.nli_name = "MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7"
        self.tokenizer = AutoTokenizer.from_pretrained(self.nli_name)
        self.nli_model = AutoModelForSequenceClassification.from_pretrained(self.nli_name)
        
        # 2. SEMANTIC EMBEDDINGS (LaBSE for cross-lingual similarity)
        self.embedder = SentenceTransformer('sentence-transformers/LaBSE')
        
        # 3. BERT SCORE
        self.bert_scorer = bert_score.BERTScorer(lang="en")
        
        # 4. NER MODELS
        self.ner_models = {
            "en": spacy.load("en_core_web_sm"),
            "xx": spacy.load("xx_ent_wiki_sm") # Multi-language fallback for Hindi/German
        }

    def detect_language(self, text: str) -> str:
        try:
            if len(text.strip()) < 3: return "en"
            return detect(text)
        except:
            return "en"
    
    def load_ner_model(self, lang: str):
        return self.ner_models.get(lang, self.ner_models["xx"])

    def _clean_numeric(self, text: str) -> List[str]:
        """Extracts raw digits for cross-script verification (e.g., â‚¹5,000 -> 5000)."""
        return re.findall(r'\d+', text.replace(',', ''))

    def semantic_entailment(self, premise: str, hypothesis: str, source_lang: str = "en", summary_lang: str = "en") -> Dict:
        inputs = self.tokenizer(premise, hypothesis, truncation=True, return_tensors="pt")
        with torch.no_grad():
            output = self.nli_model(**inputs)
        probs = torch.softmax(output.logits, dim=1).tolist()[0]
        labels = ["entailment", "neutral", "contradiction"]
        max_idx = probs.index(max(probs))
        return {"score": float(probs[max_idx]), "label": labels[max_idx]}

    def entity_alignment(self, source: str, summary: str, source_lang: str, summary_lang: str) -> Dict:
        src_nlp = self.load_ner_model(source_lang)
        sum_nlp = self.load_ner_model(summary_lang)
        
        src_ents_raw = src_nlp(source).ents
        sum_ents_raw = sum_nlp(summary).ents

        src_ents = [{"text": e.text, "label": e.label_} for e in src_ents_raw]
        sum_ents = [{"text": e.text, "label": e.label_} for e in sum_ents_raw]

        src_numbers = self._clean_numeric(source)
        sum_numbers = self._clean_numeric(summary)

        matches, matched_src_idx, matched_sum_idx = [], set(), set()
        
        # Indian Corporate Alias Map - Bridges the gap between Hindi text and English acronyms
        corp_map = {"tcs": ["à¤Ÿà¤¾à¤Ÿà¤¾ à¤•à¤‚à¤¸à¤²à¥à¤Ÿà¥‡à¤‚à¤¸à¥€ à¤¸à¤°à¥à¤µà¤¿à¤¸à¥‡à¤œ", "tata consultancy services", "à¤Ÿà¤¾à¤Ÿà¤¾", "tata"]}

        # 1. Numeric Alignment (Matches numbers even if NER fails)
        for i, s_num in enumerate(src_numbers):
            for j, u_num in enumerate(sum_numbers):
                if j in matched_sum_idx: continue
                if s_num == u_num:
                    matches.append({
                        "summary_text": u_num, 
                        "source_text": s_num, 
                        "type": "numeric",
                        "label": "CARDINAL"
                    })
                    matched_sum_idx.add(j); break

        # 2. Hard-coded Alias Bridge (This fixes the "Missing TCS" issue)
        # We manually check the map before the fuzzy NER matching
        for acronym, variants in corp_map.items():
            if acronym in summary.lower():
                for v in variants:
                    if v in source:
                        matches.append({
                            "summary_text": acronym.upper(), 
                            "source_text": v, 
                            "type": "alias",
                            "label": "ORG"
                        })
                        # Mark these as matched so they don't appear in "Missing"
                        for idx, e in enumerate(src_ents):
                            if e['text'] == v: matched_src_idx.add(idx)
                        break

        # 3. Cross-Lingual Entity & Acronym Alignment (Existing Logic)
        for i, s_ent in enumerate(src_ents):
            if i in matched_src_idx: continue
            s_text = s_ent['text'].lower()
            for j, u_ent in enumerate(sum_ents):
                if j in matched_sum_idx: continue
                u_text = u_ent['text'].lower()
                
                is_corp_match = False
                for key, variations in corp_map.items():
                    if (u_text == key or any(v in u_text for v in variations)) and \
                       (s_text == key or any(v in s_text for v in variations)):
                        is_corp_match = True

                if is_corp_match or s_text == u_text or u_text in s_text or s_text in u_text:
                    matches.append({
                        "summary_text": u_ent['text'], 
                        "source_text": s_ent['text'], 
                        "type": "entity",
                        "label": s_ent['label']
                    })
                    matched_src_idx.add(i); matched_sum_idx.add(j); break

        return {
            "matches": matches,
            "source_entities": src_ents, # REQUIRED BY PYDANTIC
            "summary_entities": sum_ents, # REQUIRED BY PYDANTIC
            "mismatches": [{"text": e['text'], "label": e['label']} for i, e in enumerate(src_ents) if i not in matched_src_idx],
            "extra_entities": [{"text": e['text'], "label": e['label']} for j, e in enumerate(sum_ents) if j not in matched_sum_idx]
        }

    def confidence_heatmap(self, source: str, summary: str) -> List[float]:
        src_sents = source.split('à¥¤') if 'à¥¤' in source else source.split('. ')
        sum_sents = summary.split('. ')
        scores = []
        for su in sum_sents:
            su_emb = self.embedder.encode([su], convert_to_tensor=True)
            similarities = [torch.cosine_similarity(su_emb, self.embedder.encode([sr], convert_to_tensor=True)).item() for sr in src_sents]
            scores.append(float(max(similarities)) if similarities else 0.0)
        return scores

    def evaluate(self, source: str, summary: str, source_lang: str = None, summary_lang: str = None) -> Dict:
        s_lang = source_lang or self.detect_language(source)
        u_lang = summary_lang or self.detect_language(summary)
        
        entailment = self.semantic_entailment(source, summary, s_lang, u_lang)
        entities = self.entity_alignment(source, summary, s_lang, u_lang)
        heatmap = self.confidence_heatmap(source, summary)
        
        # Reliability Calculation
        total_source_items = len(entities['matches']) + len(entities['mismatches'])
        match_rate = len(entities['matches']) / max(1, total_source_items)
        
        # Logic weighting: Entailment (40%) + Entity Match (60%)
        reliability = (entailment['score'] * 0.4) + (match_rate * 0.6)
        
        # Reliability Overrides for high-confidence scenarios
        if entailment['label'] == 'entailment' and match_rate > 0.7:
            reliability = max(reliability, 0.96)
        if entailment['label'] == 'contradiction':
            reliability = min(reliability, 0.25)

        return {
            "entailment": entailment,
            "entity_alignment": entities,
            "heatmap": {"sentence_scores": heatmap},
            "overall_confidence": float(reliability),
            "detected_source_lang": s_lang,
            "detected_summary_lang": u_lang,
            "executive_summary": self._generate_analysis(reliability, entities)
        }

    def _generate_analysis(self, score, ents):
        if score > 0.85: 
            return "High Fidelity: Entities and logic are well-aligned across languages."
        if score < 0.40:
            return "Risk Detected: Significant factual contradictions or missing context identified."
        return f"Warning: {len(ents['mismatches'])} key items from the source were not confirmed in the summary."